{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMRX6dFqO30g1HyOV7dYu9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduseiti/ia368v_dd_final/blob/master/test_pt0000_colbertx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "642a987d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e886f0fb"
      },
      "outputs": [],
      "source": [
        "IN_COLAB='google.colab' in sys.modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c83783",
        "outputId": "eeb5c8c8-3931-434e-cd76-d9f2d14b6337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "\n",
        "    WORKING_FOLDER=\"/content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR\"\n",
        "\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    os.chdir(WORKING_FOLDER)\n",
        "\n",
        "    !pip install transformers -q\n",
        "    !pip install mlflow -q\n",
        "    !pip install ujson -q\n",
        "    !pip install faiss-gpu -q\n",
        "    !pip install SentencePiece -q\n",
        "    !pip install faiss-gpu -q\n",
        "\n",
        "    if not os.path.exists(\"ColBERT-X\"):\n",
        "        !git clone https://github.com/hltcoe/ColBERT-X.git\n",
        "\n",
        "    PYTHON=\"python3\"\n",
        "else:\n",
        "    WORKING_FOLDER=\"/mnt/0060f889-4c27-409b-b0de-47f5427515e3/unicamp/ia368v_dd/trabalho_final/\"\n",
        "\n",
        "    PỲTHON=\"python3.8\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CLUEWEB22_FOLDER=\"clueweb22-pt\""
      ],
      "metadata": {
        "id": "WkhRgyKicBHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"ColBERT-X\")"
      ],
      "metadata": {
        "id": "1TkzYnbT-1Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m xlmr_colbert.retrieve --similarity l2 --dim 48 --query_maxlen 32 \\\n",
        "\t--index_root ../clueweb22-pt --index_name colbertx_index_pt0000 --faiss_name ivfpq.262144.faiss --part-range 0..1 \\\n",
        "\t--retrieve_only --queries ../queries_pt0000_test.tsv \\\n",
        "\t--amp --checkpoint ../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_Wh6LM4-qnh",
        "outputId": "93f7d71f-b825-4ef8-b3b4-73cdf12c3942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-13 00:53:37.923515: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-06-13 00:53:37.977375: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-13 00:53:38.947519: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "\n",
            "[Jun 13, 00:53:41] #> Creating directory /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_00.53.39 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[Jun 13, 00:53:51] #> Creating directory /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_00.53.39/logs/ \n",
            "\n",
            "\n",
            "[Jun 13, 00:53:52] {'root': 'experiments', 'experiment': 'dirty', 'run': '2023-06-13_00.53.39', 'rank': -1, 'similarity': 'l2', 'dim': 48, 'query_maxlen': 32, 'doc_maxlen': 180, 'mask_punctuation': False, 'checkpoint': '../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn', 'bsize': 128, 'amp': True, 'queries': '../queries_pt0000_test.tsv', 'collection': None, 'qrels': None, 'index_root': '../clueweb22-pt', 'index_name': 'colbertx_index_pt0000', 'partitions': None, 'nprobe': 10, 'retrieve_only': True, 'faiss_name': 'ivfpq.262144.faiss', 'faiss_depth': 1024, 'part_range': '0..1', 'batch': False, 'depth': 1000, 'log_scores': False} \n",
            "\n",
            "Downloading (…)lve/main/config.json: 100% 616/616 [00:00<00:00, 4.40MB/s]\n",
            "Downloading model.safetensors: 100% 2.24G/2.24G [00:03<00:00, 572MB/s]\n",
            "XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.30.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "Downloading (…)tencepiece.bpe.model: 100% 5.07M/5.07M [00:01<00:00, 4.77MB/s]\n",
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing ColBERT: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ColBERT were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['roberta.embeddings.position_ids', 'linear.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Jun 13, 00:54:09] #> Loading model checkpoint.\n",
            "[Jun 13, 00:54:09] #> Loading checkpoint ../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn ..\n",
            "[Jun 13, 00:55:03] #> checkpoint['epoch'] = 0\n",
            "[Jun 13, 00:55:03] #> checkpoint['batch'] = 20000\n",
            "[Jun 13, 00:55:03] [WARNING] \t Got checkpoint['arguments']['doc_maxlen'] != args.doc_maxlen (i.e., 480 != 180)\n",
            "{\n",
            "    \"root\": \"..\\/colbertx_experiments\\/\",\n",
            "    \"experiment\": \"mMSMARCO-pt_048_dim\",\n",
            "    \"run\": \"fine_tune_025\",\n",
            "    \"rank\": -1,\n",
            "    \"similarity\": \"l2\",\n",
            "    \"dim\": 48,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"doc_maxlen\": 480,\n",
            "    \"mask_punctuation\": false,\n",
            "    \"resume\": false,\n",
            "    \"resume_optimizer\": false,\n",
            "    \"checkpoint\": \"..\\/colbertx_experiments\\/mMSMARCO-pt_048_dim\\/train.py\\/fine_tune_012\\/checkpoints\\/colbert-20000.dnn\",\n",
            "    \"lr\": 3e-6,\n",
            "    \"maxsteps\": 20000,\n",
            "    \"bsize\": 12,\n",
            "    \"accumsteps\": 1,\n",
            "    \"amp\": true,\n",
            "    \"base_model\": \"xlm-roberta-large\",\n",
            "    \"triples\": \"..\\/mMARCO_triplets_025.tsv\",\n",
            "    \"queries\": null,\n",
            "    \"collection\": null\n",
            "}\n",
            "\n",
            "\n",
            "[Jun 13, 00:55:03] #> Loading the queries from ../queries_pt0000_test.tsv ...\n",
            "[Jun 13, 00:55:03] #> Got 2 queries. All QIDs are unique.\n",
            "\n",
            "[Jun 13, 00:55:05] #> Loading the FAISS index from ../clueweb22-pt/colbertx_index_pt0000/ivfpq.262144.faiss ..\n",
            "[Jun 13, 00:56:42] #> Building the emb2pid mapping..\n",
            "self.relative_range = range(0, 559240)\n",
            "[Jun 13, 00:57:14] len(self.emb2pid) = 545984134\n",
            "[Jun 13, 00:57:18] tensor.size() =  torch.Size([61057584, 48])\n",
            "[Jun 13, 00:57:18] |> Loading ../clueweb22-pt/colbertx_index_pt0000/0.pt ...\n",
            "[Jun 13, 00:58:09] #> Using strides [190, 480]..\n",
            "[Jun 13, 00:58:10] #> Logging ranked lists to /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_00.53.39/ranking.tsv\n",
            "0 como é definido o pib de um país? 545 545 26.829795837402344 499255 1076.9221782684326 ms\n",
            "1 Qual a principal influência na culinária da região nordeste do brasil? 888 888 26.767963409423828 261804 595.1671600341797 ms\n",
            "[Jun 13, 00:58:11] #> Logging query #0 (qid 10) now...\n",
            "\n",
            "\n",
            "\n",
            "/content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_00.53.39/ranking.tsv\n",
            "#> Done.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m xlmr_colbert.retrieve --similarity l2 --dim 48 --query_maxlen 32 \\\n",
        "\t--index_root ../clueweb22-pt --index_name colbertx_index_pt0000 --faiss_name ivfpq.262144.faiss --part-range 1..3 \\\n",
        "\t--retrieve_only --queries ../queries_pt0000_test.tsv \\\n",
        "\t--amp --checkpoint ../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn"
      ],
      "metadata": {
        "id": "iNP5Gzd4_01x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d48a67-cb32-49e6-a808-d05dffbb9d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-13 00:58:52.212987: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-06-13 00:58:52.269624: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-13 00:58:53.261068: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "\n",
            "[Jun 13, 00:58:54] #> Creating directory /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_00.58.54 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[Jun 13, 00:58:54] #> Creating directory /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_00.58.54/logs/ \n",
            "\n",
            "\n",
            "[Jun 13, 00:58:55] {'root': 'experiments', 'experiment': 'dirty', 'run': '2023-06-13_00.58.54', 'rank': -1, 'similarity': 'l2', 'dim': 48, 'query_maxlen': 32, 'doc_maxlen': 180, 'mask_punctuation': False, 'checkpoint': '../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn', 'bsize': 128, 'amp': True, 'queries': '../queries_pt0000_test.tsv', 'collection': None, 'qrels': None, 'index_root': '../clueweb22-pt', 'index_name': 'colbertx_index_pt0000', 'partitions': None, 'nprobe': 10, 'retrieve_only': True, 'faiss_name': 'ivfpq.262144.faiss', 'faiss_depth': 1024, 'part_range': '1..3', 'batch': False, 'depth': 1000, 'log_scores': False} \n",
            "\n",
            "XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.30.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing ColBERT: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ColBERT were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['linear.weight', 'roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Jun 13, 00:59:14] #> Loading model checkpoint.\n",
            "[Jun 13, 00:59:14] #> Loading checkpoint ../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn ..\n",
            "[Jun 13, 00:59:39] #> checkpoint['epoch'] = 0\n",
            "[Jun 13, 00:59:39] #> checkpoint['batch'] = 20000\n",
            "[Jun 13, 00:59:39] [WARNING] \t Got checkpoint['arguments']['doc_maxlen'] != args.doc_maxlen (i.e., 480 != 180)\n",
            "{\n",
            "    \"root\": \"..\\/colbertx_experiments\\/\",\n",
            "    \"experiment\": \"mMSMARCO-pt_048_dim\",\n",
            "    \"run\": \"fine_tune_025\",\n",
            "    \"rank\": -1,\n",
            "    \"similarity\": \"l2\",\n",
            "    \"dim\": 48,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"doc_maxlen\": 480,\n",
            "    \"mask_punctuation\": false,\n",
            "    \"resume\": false,\n",
            "    \"resume_optimizer\": false,\n",
            "    \"checkpoint\": \"..\\/colbertx_experiments\\/mMSMARCO-pt_048_dim\\/train.py\\/fine_tune_012\\/checkpoints\\/colbert-20000.dnn\",\n",
            "    \"lr\": 3e-6,\n",
            "    \"maxsteps\": 20000,\n",
            "    \"bsize\": 12,\n",
            "    \"accumsteps\": 1,\n",
            "    \"amp\": true,\n",
            "    \"base_model\": \"xlm-roberta-large\",\n",
            "    \"triples\": \"..\\/mMARCO_triplets_025.tsv\",\n",
            "    \"queries\": null,\n",
            "    \"collection\": null\n",
            "}\n",
            "\n",
            "\n",
            "[Jun 13, 00:59:39] #> Loading the queries from ../queries_pt0000_test.tsv ...\n",
            "[Jun 13, 00:59:39] #> Got 2 queries. All QIDs are unique.\n",
            "\n",
            "[Jun 13, 00:59:40] #> Loading the FAISS index from ../clueweb22-pt/colbertx_index_pt0000/ivfpq.262144.faiss ..\n",
            "[Jun 13, 01:00:34] #> Building the emb2pid mapping..\n",
            "self.relative_range = range(559240, 1677720)\n",
            "[Jun 13, 01:00:59] len(self.emb2pid) = 545984134\n",
            "[Jun 13, 01:01:04] tensor.size() =  torch.Size([121535837, 48])\n",
            "[Jun 13, 01:01:04] |> Loading ../clueweb22-pt/colbertx_index_pt0000/1.pt ...\n",
            "[Jun 13, 01:02:43] |> Loading ../clueweb22-pt/colbertx_index_pt0000/2.pt ...\n",
            "[Jun 13, 01:03:31] #> Using strides [190, 480]..\n",
            "[Jun 13, 01:03:32] #> Logging ranked lists to /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_00.58.54/ranking.tsv\n",
            "0 como é definido o pib de um país? 1083 1083 28.709335327148438 899350 1286.6241931915283 ms\n",
            "1 Qual a principal influência na culinária da região nordeste do brasil? 1832 1832 28.756114959716797 1395694 735.7991933822632 ms\n",
            "[Jun 13, 01:03:34] #> Logging query #0 (qid 10) now...\n",
            "\n",
            "\n",
            "\n",
            "/content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_00.58.54/ranking.tsv\n",
            "#> Done.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m xlmr_colbert.retrieve --similarity l2 --dim 48 --query_maxlen 32 \\\n",
        "\t--index_root ../clueweb22-pt --index_name colbertx_index_pt0000 --faiss_name ivfpq.262144.faiss --part-range 3..6 --log_scores \\\n",
        "\t--retrieve_only --queries ../queries_pt0000_test.tsv \\\n",
        "\t--amp --checkpoint ../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui9t-jEnQzEf",
        "outputId": "f4ce7a49-32a9-4b1f-b428-52c3a67ac357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-13 01:04:37.698545: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-06-13 01:04:37.752951: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-13 01:04:38.970591: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "\n",
            "[Jun 13, 01:04:56] #> Creating directory /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.04.40 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[Jun 13, 01:05:02] #> Creating directory /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.04.40/logs/ \n",
            "\n",
            "\n",
            "[Jun 13, 01:05:02] {'root': 'experiments', 'experiment': 'dirty', 'run': '2023-06-13_01.04.40', 'rank': -1, 'similarity': 'l2', 'dim': 48, 'query_maxlen': 32, 'doc_maxlen': 180, 'mask_punctuation': False, 'checkpoint': '../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn', 'bsize': 128, 'amp': True, 'queries': '../queries_pt0000_test.tsv', 'collection': None, 'qrels': None, 'index_root': '../clueweb22-pt', 'index_name': 'colbertx_index_pt0000', 'partitions': None, 'nprobe': 10, 'retrieve_only': True, 'faiss_name': 'ivfpq.262144.faiss', 'faiss_depth': 1024, 'part_range': '3..6', 'batch': False, 'depth': 1000, 'log_scores': True} \n",
            "\n",
            "XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.30.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing ColBERT: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ColBERT were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['linear.weight', 'roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Jun 13, 01:05:23] #> Loading model checkpoint.\n",
            "[Jun 13, 01:05:23] #> Loading checkpoint ../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn ..\n",
            "[Jun 13, 01:06:09] #> checkpoint['epoch'] = 0\n",
            "[Jun 13, 01:06:09] #> checkpoint['batch'] = 20000\n",
            "[Jun 13, 01:06:09] [WARNING] \t Got checkpoint['arguments']['doc_maxlen'] != args.doc_maxlen (i.e., 480 != 180)\n",
            "{\n",
            "    \"root\": \"..\\/colbertx_experiments\\/\",\n",
            "    \"experiment\": \"mMSMARCO-pt_048_dim\",\n",
            "    \"run\": \"fine_tune_025\",\n",
            "    \"rank\": -1,\n",
            "    \"similarity\": \"l2\",\n",
            "    \"dim\": 48,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"doc_maxlen\": 480,\n",
            "    \"mask_punctuation\": false,\n",
            "    \"resume\": false,\n",
            "    \"resume_optimizer\": false,\n",
            "    \"checkpoint\": \"..\\/colbertx_experiments\\/mMSMARCO-pt_048_dim\\/train.py\\/fine_tune_012\\/checkpoints\\/colbert-20000.dnn\",\n",
            "    \"lr\": 3e-6,\n",
            "    \"maxsteps\": 20000,\n",
            "    \"bsize\": 12,\n",
            "    \"accumsteps\": 1,\n",
            "    \"amp\": true,\n",
            "    \"base_model\": \"xlm-roberta-large\",\n",
            "    \"triples\": \"..\\/mMARCO_triplets_025.tsv\",\n",
            "    \"queries\": null,\n",
            "    \"collection\": null\n",
            "}\n",
            "\n",
            "\n",
            "[Jun 13, 01:06:09] #> Loading the queries from ../queries_pt0000_test.tsv ...\n",
            "[Jun 13, 01:06:10] #> Got 2 queries. All QIDs are unique.\n",
            "\n",
            "[Jun 13, 01:06:11] #> Loading the FAISS index from ../clueweb22-pt/colbertx_index_pt0000/ivfpq.262144.faiss ..\n",
            "[Jun 13, 01:07:46] #> Building the emb2pid mapping..\n",
            "self.relative_range = range(1677720, 3355440)\n",
            "[Jun 13, 01:08:13] len(self.emb2pid) = 545984134\n",
            "[Jun 13, 01:08:19] tensor.size() =  torch.Size([183024700, 48])\n",
            "[Jun 13, 01:08:19] |> Loading ../clueweb22-pt/colbertx_index_pt0000/3.pt ...\n",
            "[Jun 13, 01:09:14] |> Loading ../clueweb22-pt/colbertx_index_pt0000/4.pt ...\n",
            "[Jun 13, 01:09:59] |> Loading ../clueweb22-pt/colbertx_index_pt0000/5.pt ...\n",
            "[Jun 13, 01:10:43] #> Using strides [191, 480]..\n",
            "[Jun 13, 01:10:44] #> Logging ranked lists to /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.04.40/ranking.tsv\n",
            "0 como é definido o pib de um país? 1722 1722 27.776716232299805 2716600 976.1567115783691 ms\n",
            "1 Qual a principal influência na culinária da região nordeste do brasil? 2569 2569 27.947528839111328 3270143 544.2218780517578 ms\n",
            "[Jun 13, 01:10:46] #> Logging query #0 (qid 10) now...\n",
            "\n",
            "\n",
            "\n",
            "/content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.04.40/ranking.tsv\n",
            "#> Done.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m xlmr_colbert.retrieve --similarity l2 --dim 48 --query_maxlen 32 \\\n",
        "\t--index_root ../clueweb22-pt --index_name colbertx_index_pt0000 --faiss_name ivfpq.262144.faiss --part-range 0..3 --log_scores \\\n",
        "\t--retrieve_only --queries ../queries_pt0000_test.tsv \\\n",
        "\t--amp --checkpoint ../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZAp84T0SFwu",
        "outputId": "53d5ac76-3376-483f-92f5-f0e40137b6a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-13 01:12:05.346501: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-06-13 01:12:05.402378: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-13 01:12:06.620239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "\n",
            "[Jun 13, 01:12:23] #> Creating directory /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.12.08 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[Jun 13, 01:12:30] #> Creating directory /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.12.08/logs/ \n",
            "\n",
            "\n",
            "[Jun 13, 01:12:30] {'root': 'experiments', 'experiment': 'dirty', 'run': '2023-06-13_01.12.08', 'rank': -1, 'similarity': 'l2', 'dim': 48, 'query_maxlen': 32, 'doc_maxlen': 180, 'mask_punctuation': False, 'checkpoint': '../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn', 'bsize': 128, 'amp': True, 'queries': '../queries_pt0000_test.tsv', 'collection': None, 'qrels': None, 'index_root': '../clueweb22-pt', 'index_name': 'colbertx_index_pt0000', 'partitions': None, 'nprobe': 10, 'retrieve_only': True, 'faiss_name': 'ivfpq.262144.faiss', 'faiss_depth': 1024, 'part_range': '0..3', 'batch': False, 'depth': 1000, 'log_scores': True} \n",
            "\n",
            "XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.30.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing ColBERT: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ColBERT were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['linear.weight', 'roberta.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Jun 13, 01:12:49] #> Loading model checkpoint.\n",
            "[Jun 13, 01:12:49] #> Loading checkpoint ../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn ..\n",
            "[Jun 13, 01:13:34] #> checkpoint['epoch'] = 0\n",
            "[Jun 13, 01:13:34] #> checkpoint['batch'] = 20000\n",
            "[Jun 13, 01:13:34] [WARNING] \t Got checkpoint['arguments']['doc_maxlen'] != args.doc_maxlen (i.e., 480 != 180)\n",
            "{\n",
            "    \"root\": \"..\\/colbertx_experiments\\/\",\n",
            "    \"experiment\": \"mMSMARCO-pt_048_dim\",\n",
            "    \"run\": \"fine_tune_025\",\n",
            "    \"rank\": -1,\n",
            "    \"similarity\": \"l2\",\n",
            "    \"dim\": 48,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"doc_maxlen\": 480,\n",
            "    \"mask_punctuation\": false,\n",
            "    \"resume\": false,\n",
            "    \"resume_optimizer\": false,\n",
            "    \"checkpoint\": \"..\\/colbertx_experiments\\/mMSMARCO-pt_048_dim\\/train.py\\/fine_tune_012\\/checkpoints\\/colbert-20000.dnn\",\n",
            "    \"lr\": 3e-6,\n",
            "    \"maxsteps\": 20000,\n",
            "    \"bsize\": 12,\n",
            "    \"accumsteps\": 1,\n",
            "    \"amp\": true,\n",
            "    \"base_model\": \"xlm-roberta-large\",\n",
            "    \"triples\": \"..\\/mMARCO_triplets_025.tsv\",\n",
            "    \"queries\": null,\n",
            "    \"collection\": null\n",
            "}\n",
            "\n",
            "\n",
            "[Jun 13, 01:13:34] #> Loading the queries from ../queries_pt0000_test.tsv ...\n",
            "[Jun 13, 01:13:35] #> Got 2 queries. All QIDs are unique.\n",
            "\n",
            "[Jun 13, 01:13:37] #> Loading the FAISS index from ../clueweb22-pt/colbertx_index_pt0000/ivfpq.262144.faiss ..\n",
            "[Jun 13, 01:15:09] #> Building the emb2pid mapping..\n",
            "self.relative_range = range(0, 1677720)\n",
            "[Jun 13, 01:15:37] len(self.emb2pid) = 545984134\n",
            "[Jun 13, 01:15:42] tensor.size() =  torch.Size([182592909, 48])\n",
            "[Jun 13, 01:15:42] |> Loading ../clueweb22-pt/colbertx_index_pt0000/0.pt ...\n",
            "[Jun 13, 01:16:37] |> Loading ../clueweb22-pt/colbertx_index_pt0000/1.pt ...\n",
            "[Jun 13, 01:20:41] |> Loading ../clueweb22-pt/colbertx_index_pt0000/2.pt ...\n",
            "[Jun 13, 01:21:21] #> Using strides [190, 480]..\n",
            "[Jun 13, 01:21:22] #> Logging ranked lists to /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.12.08/ranking.tsv\n",
            "0 como é definido o pib de um país? 1628 1628 28.709335327148438 899350 984.3857288360596 ms\n",
            "1 Qual a principal influência na culinária da região nordeste do brasil? 2720 2720 28.756114959716797 1395694 548.8842725753784 ms\n",
            "[Jun 13, 01:21:23] #> Logging query #0 (qid 10) now...\n",
            "\n",
            "\n",
            "\n",
            "/content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.12.08/ranking.tsv\n",
            "#> Done.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m xlmr_colbert.retrieve --similarity l2 --dim 48 --query_maxlen 32 \\\n",
        "\t--index_root ../clueweb22-pt --index_name colbertx_index_pt0000 --faiss_name ivfpq.262144.faiss --part-range 6..9 --log_scores \\\n",
        "\t--retrieve_only --queries ../queries_pt0000_test.tsv \\\n",
        "\t--amp --checkpoint ../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lymr_hfKTzQY",
        "outputId": "9b46e6b2-a776-4ea4-8bfd-7ac28a5512d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-13 01:22:38.805426: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-06-13 01:22:38.893197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-13 01:22:40.056304: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "\n",
            "[Jun 13, 01:22:58] #> Creating directory /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.22.41 \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[Jun 13, 01:23:04] #> Creating directory /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.22.41/logs/ \n",
            "\n",
            "\n",
            "[Jun 13, 01:23:04] {'root': 'experiments', 'experiment': 'dirty', 'run': '2023-06-13_01.22.41', 'rank': -1, 'similarity': 'l2', 'dim': 48, 'query_maxlen': 32, 'doc_maxlen': 180, 'mask_punctuation': False, 'checkpoint': '../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn', 'bsize': 128, 'amp': True, 'queries': '../queries_pt0000_test.tsv', 'collection': None, 'qrels': None, 'index_root': '../clueweb22-pt', 'index_name': 'colbertx_index_pt0000', 'partitions': None, 'nprobe': 10, 'retrieve_only': True, 'faiss_name': 'ivfpq.262144.faiss', 'faiss_depth': 1024, 'part_range': '6..9', 'batch': False, 'depth': 1000, 'log_scores': True} \n",
            "\n",
            "XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-large\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.30.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing ColBERT: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing ColBERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ColBERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ColBERT were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['roberta.embeddings.position_ids', 'linear.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[Jun 13, 01:23:23] #> Loading model checkpoint.\n",
            "[Jun 13, 01:23:23] #> Loading checkpoint ../colbertx_experiments/mMSMARCO-pt_048_dim/train.py/fine_tune_025/checkpoints/colbert-20000.dnn ..\n",
            "[Jun 13, 01:24:09] #> checkpoint['epoch'] = 0\n",
            "[Jun 13, 01:24:09] #> checkpoint['batch'] = 20000\n",
            "[Jun 13, 01:24:09] [WARNING] \t Got checkpoint['arguments']['doc_maxlen'] != args.doc_maxlen (i.e., 480 != 180)\n",
            "{\n",
            "    \"root\": \"..\\/colbertx_experiments\\/\",\n",
            "    \"experiment\": \"mMSMARCO-pt_048_dim\",\n",
            "    \"run\": \"fine_tune_025\",\n",
            "    \"rank\": -1,\n",
            "    \"similarity\": \"l2\",\n",
            "    \"dim\": 48,\n",
            "    \"query_maxlen\": 32,\n",
            "    \"doc_maxlen\": 480,\n",
            "    \"mask_punctuation\": false,\n",
            "    \"resume\": false,\n",
            "    \"resume_optimizer\": false,\n",
            "    \"checkpoint\": \"..\\/colbertx_experiments\\/mMSMARCO-pt_048_dim\\/train.py\\/fine_tune_012\\/checkpoints\\/colbert-20000.dnn\",\n",
            "    \"lr\": 3e-6,\n",
            "    \"maxsteps\": 20000,\n",
            "    \"bsize\": 12,\n",
            "    \"accumsteps\": 1,\n",
            "    \"amp\": true,\n",
            "    \"base_model\": \"xlm-roberta-large\",\n",
            "    \"triples\": \"..\\/mMARCO_triplets_025.tsv\",\n",
            "    \"queries\": null,\n",
            "    \"collection\": null\n",
            "}\n",
            "\n",
            "\n",
            "[Jun 13, 01:24:09] #> Loading the queries from ../queries_pt0000_test.tsv ...\n",
            "[Jun 13, 01:24:11] #> Got 2 queries. All QIDs are unique.\n",
            "\n",
            "[Jun 13, 01:24:13] #> Loading the FAISS index from ../clueweb22-pt/colbertx_index_pt0000/ivfpq.262144.faiss ..\n",
            "[Jun 13, 01:25:48] #> Building the emb2pid mapping..\n",
            "self.relative_range = range(3355440, 5000015)\n",
            "[Jun 13, 01:26:18] len(self.emb2pid) = 545984134\n",
            "[Jun 13, 01:26:23] tensor.size() =  torch.Size([180368061, 48])\n",
            "[Jun 13, 01:26:23] |> Loading ../clueweb22-pt/colbertx_index_pt0000/6.pt ...\n",
            "[Jun 13, 01:27:09] |> Loading ../clueweb22-pt/colbertx_index_pt0000/7.pt ...\n",
            "[Jun 13, 01:27:51] |> Loading ../clueweb22-pt/colbertx_index_pt0000/8.pt ...\n",
            "[Jun 13, 01:28:30] #> Using strides [193, 480]..\n",
            "[Jun 13, 01:28:31] #> Logging ranked lists to /content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.22.41/ranking.tsv\n",
            "0 como é definido o pib de um país? 1631 1631 27.791790008544922 4386552 987.6437187194824 ms\n",
            "1 Qual a principal influência na culinária da região nordeste do brasil? 2783 2783 28.2354793548584 4168347 564.0531778335571 ms\n",
            "[Jun 13, 01:28:32] #> Logging query #0 (qid 10) now...\n",
            "\n",
            "\n",
            "\n",
            "/content/drive/MyDrive/unicamp/ia368v_dd/trabalho_final_UNICAMP-IR/ColBERT-X/experiments/dirty/retrieve.py/2023-06-13_01.22.41/ranking.tsv\n",
            "#> Done.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q2TsNNvsWN4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}